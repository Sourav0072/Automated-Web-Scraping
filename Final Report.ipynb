{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac2bfabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d62aa2",
   "metadata": {},
   "source": [
    "# NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nviewmed.com/news\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "output = []\n",
    "\n",
    "# Find all the news headings\n",
    "headings = soup.find_all(\"h2\")\n",
    "\n",
    "# Extract the text from each heading and append it to the output list\n",
    "for heading in headings:\n",
    "    title = heading.text.strip()\n",
    "    output.append(title)\n",
    "\n",
    "# Get the current date\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save the output to a CSV file with the current date in the filename\n",
    "filename = f\"news_headings_{today}.csv\"\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Headings\"])  # Write the header\n",
    "    writer.writerows([[heading] for heading in output])  # Write the data rows\n",
    "\n",
    "print(f\"News headings saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af8a9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to holmusk-news-events-data_2023-07-14.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.holmusk.com/news\"\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "results = soup.find('div', class_=\"collection-list _3-col _2-col-tablet w-dyn-items\")\n",
    "results = results.find_all('div', class_=\"w-dyn-item\")\n",
    "\n",
    "output = []\n",
    "\n",
    "for result in results:\n",
    "    link = result.find('a', class_=\"link-block-2 w-inline-block\")\n",
    "    link = link.attrs['href']\n",
    "\n",
    "    test_cond = str(link)\n",
    "    if \"holmusk.com\" in test_cond:\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        results = soup.find_all('div', class_=\"container news-template w-container\")\n",
    "\n",
    "        for result in results:\n",
    "            title = result.find(class_=\"news-title\")\n",
    "            description = result.find(\"div\", class_=\"w-richtext\")\n",
    "            \n",
    "            news = {\n",
    "                \"News Title\": title.text,\n",
    "                \"Details\": description.text\n",
    "            }\n",
    "\n",
    "            output.append(news)\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"holmusk-news-events-data_{current_date}.csv\"\n",
    "\n",
    "# Save the output as CSV\n",
    "csv_columns = [\"News Title\", \"Details\"]\n",
    "\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output)\n",
    "\n",
    "print(f\"Data saved to {filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70625c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to headings_and_texts_2023-07-14.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get the HTML content of the website\n",
    "response = requests.get(\"https://www.owl.health/newsroom/\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# Find all the headings\n",
    "headings = soup.find_all(\"h2\")\n",
    "# Find all the text under the headings\n",
    "texts = soup.find_all(\"p\")\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"headings_and_texts_{current_date}.csv\"\n",
    "\n",
    "# Create a CSV file\n",
    "with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    # Write the header row\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Heading\", \"Text\"])\n",
    "    # Write the data rows\n",
    "    for heading, text in zip(headings, texts):\n",
    "        writer.writerow([heading.text, text.text])\n",
    "\n",
    "print(f\"Data saved to {filename} successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149ecd7",
   "metadata": {},
   "source": [
    "# BLOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7abdb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved in blog_data_2023-07-14.csv\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL\n",
    "url = \"https://www.silvercloudhealth.com/uk/blog#default:selected=true|paging:currentPage=0|paging:number=9\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the blog post items\n",
    "articles = soup.find_all(\"a\", class_=\"c-header-mega__link\")\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"blog_data_{current_date}.csv\"\n",
    "\n",
    "# Create a CSV file and write the header row\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Blog Title\", \"Blog Description\"])\n",
    "\n",
    "    # Loop over the blog post items and extract the title and description\n",
    "    for article in articles:\n",
    "        title = article.find(\"h4\").text.strip()\n",
    "        description = article.find(\"p\").text.strip()\n",
    "        \n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([title, description])\n",
    "\n",
    "print(\"Data has been saved in\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9abc232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to blueprint-health-blogs_2023-07-14.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "blog_url = \"https://www.blueprint-health.com/blog\"\n",
    "blog_page = requests.get(blog_url).text\n",
    "blog_doc = BeautifulSoup(blog_page, 'html.parser')\n",
    "blog_pages = int(blog_doc.find(class_='w-pagination-wrapper').a['href'].split('=')[-1])\n",
    "\n",
    "output = []\n",
    "\n",
    "for page in range(1, blog_pages + 1):\n",
    "    blog_url = f\"https://www.blueprint-health.com/blog?cbb911e6_page={page}\"\n",
    "    blog_page = requests.get(blog_url).text\n",
    "    blog_doc = BeautifulSoup(blog_page, 'html.parser')\n",
    "    items = blog_doc.find_all(class_=\"collection-item-blog\")\n",
    "\n",
    "    for item in items:\n",
    "        title = item.find(class_=\"title-post\").text\n",
    "        get_link = item.find('a')\n",
    "        link = \"https://www.blueprint-health.com\" + get_link.attrs['href']\n",
    "        get_details = requests.get(link).text\n",
    "        detail_page = BeautifulSoup(get_details, 'html.parser')\n",
    "        details = detail_page.find(class_=\"rich-text-blog\").text\n",
    "\n",
    "        blog = {\n",
    "            \"Title\": title,\n",
    "            \"Description\": details\n",
    "        }\n",
    "\n",
    "        output.append(blog)\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"blueprint-health-blogs_{current_date}.csv\"\n",
    "\n",
    "# Save the output as CSV\n",
    "csv_columns = [\"Title\", \"Description\"]\n",
    "\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output)\n",
    "    \n",
    "print(f\"Data saved to {filename} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36be434",
   "metadata": {},
   "source": [
    "# RESEARCH PUBLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e3a5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully in neuroblu-announcements-2023-07-14.csv\n"
     ]
    }
   ],
   "source": [
    "req = Request(\"https://www.neuroflow.com/research/\", headers={'User-Agent': 'XYZ/3.0'})\n",
    "webpage = urlopen(req, timeout=10).read()\n",
    "research_doc = BeautifulSoup(webpage, 'html.parser')\n",
    "items = research_doc.find_all(\"div\", class_=\"vc_row wpb_row vc_row-fluid vc_column-gap-25\")\n",
    "\n",
    "output = []\n",
    "for item in items:\n",
    "    details = item.find(\"div\", class_=\"w-author-desc\")\n",
    "    title = item.find(\"h3\")\n",
    "\n",
    "    research_title = title.text.strip() if title else \"Title not available\"\n",
    "    research_description = details.text.strip() if details else \"Description not available\"\n",
    "\n",
    "    output.append({\"Research Title\": research_title, \"Research Description\": research_description})\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"neuroflow-research_{current_date}.csv\"\n",
    "\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fieldnames = [\"Research Title\", \"Research Description\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output)\n",
    "\n",
    "print(f\"Data saved successfully in {filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73117b80",
   "metadata": {},
   "source": [
    "# ANNOUCEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe0cfc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in CSV format: neuroblu-announcements-2023-07-14.csv\n"
     ]
    }
   ],
   "source": [
    "req = Request(\"https://www.neuroblu.ai/announcements/\", headers={'User-Agent': 'XYZ/3.0'})\n",
    "webpage = urlopen(req, timeout=10).read()\n",
    "announcement_doc = BeautifulSoup(webpage, 'html.parser')\n",
    "items = announcement_doc.find_all(\"div\", class_=\"collection-item-4 w-dyn-item\")\n",
    "\n",
    "output = []\n",
    "for item in items:\n",
    "    title = item.find('h3', class_=\"heading-3 nrelease announcements\")\n",
    "    get_link = item.find('a')\n",
    "    link = 'https://www.neuroblu.ai' + get_link.attrs['href']\n",
    "    headers = {'User-Agent': 'XYZ/3.0'}\n",
    "    response = requests.get(link, headers=headers).text\n",
    "    detail_page = BeautifulSoup(response, 'html.parser')\n",
    "    details = detail_page.find(\"div\", class_=\"rich-text-block w-richtext\")\n",
    "    announcement = {\n",
    "        \"Announcement Title\": title.text,\n",
    "        \"Announcement Description\": details.text if details else \"\"\n",
    "    }\n",
    "    output.append(announcement)\n",
    "\n",
    "# Generate the filename with the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"neuroblu-announcements-{current_date}.csv\"\n",
    "\n",
    "# Save the output in CSV format\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=output[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output)\n",
    "\n",
    "print(\"Data saved in CSV format:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f06e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
